+++
title =  "I don’t really use OSX anymore"
date = 2010-02-09T01:15:00-08:00
tags = ["programming", "mac", "apple"]
featured_image = ""
description = ""
+++

I have two desktop systems, side-by-side: an Intel Mac Mini and an Intel 21" iMac. The Mini runs Leopard and the iMac runs Ubuntu Karmic Koala, and I find myself completely satisfied with the Linux desktop, and switch back to OSX as an auxiliary rather than as my primary.

I started on Debian back in 1999. I wanted to get into Linux, but both Red Hat and SuSE were a little hard to get going for beginners, and the packages supplied were always a weird grab bag of old and new. I remember KDE 2 was awesome, but I don’t remember much past that. A friend and coworker introduced me to Debian and I was amazed at how much more intuitive it was to use. It didn’t leave me out in the cold to fend for myself, but it automated enough of the low-levels of Linux that it made the rest seem approachable. And its default configuration was super lightweight and not memory hungry at all. I was sold.

I started using Apple hardware again six years ago, on my 21st birthday. Right at the “Apple Renaissance” for developers I bought myself a 12" Aluminum Powerbook and it got me through my last two years of college and the year and a half after it of doing research and my first full-time job. It was probably the most innovative computer I’d ever owned to that point – it had a Mini-DVI port that I could use to have a dual headed system (which was the coolest thing ever) and Bluetooth, which meant I could sync my contacts and calendars with my Sony Ericsson cell phone. I can’t tell you how many times I made it to class only because my phone beeped at me 5 minutes before lecture started. It was self contained: I could do all my C++/Python/Java work for CS courses and contract work without installing anything, it had word processors and diagramming software that handled international text for my linguistics courses, it was a perfect computing device for me at the time.

My love affair continued because of OSX desktop software – it was a pretty face with usable apps on a standard UNIX system: cron and friends were all there for the using. It even had some new technologies that were really exciting: I still consider MDNS-SD one of the greatest consumer networking advancements ever. I could install software in an apt-like way with fink or ports. It always felt like there was a little friction there with a near-parallel UNIX system on a UNIX system when I ran these utilities, but it was good enough.

As OSX aged, it began to diverge from the standard (notably, launchd made me take notice) and felt less open. What was once a pretty face on a standard UNIX became a slightly prettier face on an increasingly non-standard operating system. I had to re-learn the same things over and over from release to release as they began to deprecate the old tools and methods for doing things. Again, the notable example here is launchd, which replaced `init.d` and `cron`. While the Quartz window manager and rendering seemed great in 2001, the advancements in X like Cairo and Compiz have caught up feature-and-performance-wise and are open and cross-platform. Apple’s slowly adding not-so-open utilities to scrape up edge cases like GrandCentral, which is kind of an indicator that the desktop environment itself is pretty much stagnant and Apple’s innovation is going to go into other semi-proprietary technologies on top of their existing stack.

Installing software is obviously much easier in OSX (discounting apt): I drag an icon to /Applications. If a title isn’t in apt, or is outdated in the Ubuntu repositories, it’s a pain to get going. But more and more apps are providing .debs (with daily builds!), or better yet, are showing up in the Ubuntu PPAs and this is becoming less of an issue. I’m also learning to live with 6 month old software: it’ll get in when it gets in, I’ll live with the features it has now. Usually the bleeding edge has APIs that are so in flux that it makes absolutely no sense to develop against them.

So I find myself on Ubuntu. The UI is familiar and at this point it’s easier to test and deploy apps onto a Debian server using the system that is more Debian-like than the one that is not. It’s fast, it’s responsive, and while it can be idiomatic in its own right for a developer who mostly uses vi, Python and C, it’s great. I can’t really heap too much praise or criticism on it, it just is there and gets out of the way. Ubuntu sucked for stability in 2006, but it works well now. It lets me get stuff done. The new “features” like Ubuntu One that are being constantly tacked on stay out of the way and can even be uninstalled. I had to install a third-party hack to get rid of the Mobile Me account thing in the Finder.

This isn’t to say I’ve had a falling out with the Mac: just having used it for 6 years now, it seems a lot less exciting and a lot more of a hassle to use than it used to. I’m slowly using it less and less and while I will likely always have OSX and Windows as a desktop install somewhere, it won’t necessarily be my go-to platform to get things done. So from a highly subjective point of view, I seem to be unconsciously preferring Linux to OSX to do my daily computing chores, and the trend seems to be strengthening every day.
